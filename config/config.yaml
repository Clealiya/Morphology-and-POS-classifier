name: experiment                # name of the experiment (that will be change by {task_name}_{config.data.language} )
save_experiment: true           # save the experiment or not

# data options
data:
    path: data                  # path to the data that contains UD_Abaza-ATD, UD_Afrikaans-AfriBooms, ...
    language: French            # language
    sequence_length: 10         # number of word in the sequence
    pad: <PAD>                  # padding caracter
    unk: <UNK>                  # unkown caracter
    sequence_function: dummy    # function to split sequences
    indexes: [1, 3, 5]          # indexes to take from data. must have 1 for word, 3 for POS and 5 for morphy
    vocab:
        path: dictionary        # path to the vocab dictionnary
        unk_rate: 0.01          # rate of words dropout
        save: false             # save the vocab or not
        num_words: 33992        # number of words (that will change depending of the language and unk_rate)

# task options
task:
    task_name: get_morphy      # task do to: must be get_pos or get_morphy
    get_pos_info:
        num_classes: 19         # number of POS classes
    get_morphy_info:
        num_classes: 28         # number of MORPHY classes
        num_features: 13        # number of max classes possibility
        use_pos: true              # use POS to get MORPHY

# model options
model:
    # model to get POS with LSTM
    lstm_pos:
        lstm_hidd_size_1: 64        # number of feature in the first LSTM layers
        lstm_hidd_size_2: null      # number of feature in the second LSTM layers (null no second LSTM layer)
        fc_hidd_size: []            # list of neronnes in dense layers. Doesn't count the last layars with give the logits
        embedding_size: 64          # embedding size
        bidirectional: true         # LSTM bidirectional
        activation: relu            # activation function between layers (not at the end)
        dropout: 0.1                # rate neuronnes dropout
    
    # model to get MORPHY with LSTM
    lstm_morphy:
        lstm_hidd_size_1: 64        # number of feature in the first LSTM layers
        lstm_hidd_size_2: 128       # number of feature in the second LSTM layers (null no second LSTM layer)
        fc_hidd_size: [128, 64]     # list of neronnes in dense layers
        embedding_size: 64          # embedding size
        bidirectional: true         # LSTM bidirectional
        activation: relu            # activation function between layers (not at the end)
        dropout: 0.1                # rate neuronnes dropout
        separate: true              # separate classes at the end. False -> super tag
        add_zero: true              # contatene with zero at the end (only with separete==True)
        


# learning options
learning:
    loss: crossentropy
    optimizer: adam
    learning_rate: 0.01
    milesstone: [10, 20]
    gamma: 0.5
    epochs: 70
    batch_size: 2048
    shuffle: true
    drop_last: true
    save_checkpoint: true
    device: cuda

# metrics options
metrics:
    acc: true                   # accuracy micro (and macro for POS)
    allgood: true               # (only for MORPHY) all classes is good for words